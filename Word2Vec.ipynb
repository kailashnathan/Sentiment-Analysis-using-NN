{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kailash Nathan\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk.data  \n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('ReviewsClean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=dataset[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars=data['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews=data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000L,), (100000L,), (4037798, 2))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars.shape,reviews.shape,dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_te, Y_tr, Y_te = train_test_split(reviews,stars, test_size=0.1,shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000L,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4037798, 2), (90000L,), (90000L,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape,X_tr.shape,Y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000L,), (10000L,), (10000L,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars.shape,X_te.shape,Y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    #raw_sentences1 = tokenizer.tokenize(review)\n",
    "    #print \"old_len:\",len(raw_sentences1)\n",
    "    #raw_sentences = review.split(\"\\n\")\n",
    "    #print \"new_len:\",len(raw_sentences)\n",
    "    # 2. Break sentences into separate \n",
    "    #raw_sentences = raw_sentences.split(\"\\n\")\n",
    "    #\n",
    "    # 3. Loop over each sentence\n",
    "    sentences = []\n",
    "    counter = 0\n",
    "    for raw_sentence in review:\n",
    "        counter += 1\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "        if (counter%10000 == 0):\n",
    "            print (\"Done with %d Reviews\"%counter)\n",
    "        \n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "review_to_sentences() takes at least 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-b1207ef32c0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreview_to_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: review_to_sentences() takes at least 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "review_to_sentences(X_tr.values[2:4],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords == 0:\n",
    "        return featureVec \n",
    "    else:\n",
    "        return np.divide(featureVec,nwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%10000 == 0:\n",
    "           print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "           \n",
    "           \n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All set with the files now\n",
      "Parsing sentences from training set\n",
      "Done with 10000 Reviews\n",
      "Done with 20000 Reviews\n",
      "Done with 30000 Reviews\n",
      "Done with 40000 Reviews\n",
      "Done with 50000 Reviews\n",
      "Done with 60000 Reviews\n",
      "Done with 70000 Reviews\n",
      "Done with 80000 Reviews\n",
      "Done with 90000 Reviews\n"
     ]
    }
   ],
   "source": [
    "# Initialize paths for training data set\n",
    "\n",
    "\n",
    "#train_labels = \"traindata.labels\"\n",
    "\n",
    "#Open Training files, get content and close files\n",
    "#tr_labels = ftr_l.read()\n",
    "#ftr_d.close()\n",
    "#ftr_l.close()\n",
    "\n",
    "print (\"All set with the files now\")\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "print (\"Parsing sentences from training set\")\n",
    "\n",
    "#print X_tr.values[9]\n",
    "sentences = review_to_sentences(X_tr.values,tokenizer)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000 90000\n"
     ]
    }
   ],
   "source": [
    "print len(sentences),len(X_tr.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-07 20:04:02,686 : INFO : collecting all words and their counts\n",
      "2017-12-07 20:04:02,687 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-07 20:04:03,167 : INFO : PROGRESS: at sentence #10000, processed 1140958 words, keeping 25959 word types\n",
      "2017-12-07 20:04:03,641 : INFO : PROGRESS: at sentence #20000, processed 2291219 words, keeping 35263 word types\n",
      "2017-12-07 20:04:04,059 : INFO : PROGRESS: at sentence #30000, processed 3420525 words, keeping 42001 word types\n",
      "2017-12-07 20:04:04,404 : INFO : PROGRESS: at sentence #40000, processed 4552917 words, keeping 47458 word types\n",
      "2017-12-07 20:04:04,852 : INFO : PROGRESS: at sentence #50000, processed 5686041 words, keeping 51978 word types\n",
      "2017-12-07 20:04:05,213 : INFO : PROGRESS: at sentence #60000, processed 6823209 words, keeping 56004 word types\n",
      "2017-12-07 20:04:05,690 : INFO : PROGRESS: at sentence #70000, processed 7956870 words, keeping 59760 word types\n",
      "2017-12-07 20:04:06,210 : INFO : PROGRESS: at sentence #80000, processed 9098924 words, keeping 63298 word types\n",
      "2017-12-07 20:04:06,486 : INFO : collected 66580 word types from a corpus of 10233569 raw words and 90000 sentences\n",
      "2017-12-07 20:04:06,487 : INFO : Loading a fresh vocabulary\n",
      "2017-12-07 20:04:06,575 : INFO : min_count=40 retains 8416 unique words (12% of original 66580, drops 58164)\n",
      "2017-12-07 20:04:06,576 : INFO : min_count=40 leaves 9946250 word corpus (97% of original 10233569, drops 287319)\n",
      "2017-12-07 20:04:06,609 : INFO : deleting the raw counts dictionary of 66580 items\n",
      "2017-12-07 20:04:06,615 : INFO : sample=0.001 downsamples 58 most-common words\n",
      "2017-12-07 20:04:06,615 : INFO : downsampling leaves estimated 7271546 word corpus (73.1% of prior 9946250)\n",
      "2017-12-07 20:04:06,618 : INFO : estimated required memory for 8416 words and 300 dimensions: 24406400 bytes\n",
      "2017-12-07 20:04:06,661 : INFO : resetting layer weights\n",
      "2017-12-07 20:04:06,897 : INFO : training model with 4 workers on 8416 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-12-07 20:04:08,867 : INFO : PROGRESS: at 1.32% examples, 482625 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:09,901 : INFO : PROGRESS: at 3.12% examples, 561202 words/s, in_qsize 8, out_qsize 1\n",
      "2017-12-07 20:04:10,911 : INFO : PROGRESS: at 4.86% examples, 584314 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:11,921 : INFO : PROGRESS: at 6.41% examples, 577376 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:12,917 : INFO : PROGRESS: at 7.43% examples, 536714 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:13,924 : INFO : PROGRESS: at 8.57% examples, 514692 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:14,948 : INFO : PROGRESS: at 9.84% examples, 504963 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:15,980 : INFO : PROGRESS: at 11.28% examples, 504943 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:16,982 : INFO : PROGRESS: at 12.67% examples, 504993 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:17,984 : INFO : PROGRESS: at 14.13% examples, 507112 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:18,996 : INFO : PROGRESS: at 15.59% examples, 509375 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:20,003 : INFO : PROGRESS: at 16.84% examples, 504787 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:21,022 : INFO : PROGRESS: at 18.10% examples, 500425 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:22,029 : INFO : PROGRESS: at 19.12% examples, 490922 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:23,039 : INFO : PROGRESS: at 20.53% examples, 492010 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:24,046 : INFO : PROGRESS: at 21.91% examples, 492229 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:25,055 : INFO : PROGRESS: at 23.47% examples, 496870 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:26,068 : INFO : PROGRESS: at 24.74% examples, 494804 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:27,084 : INFO : PROGRESS: at 25.63% examples, 485515 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:28,079 : INFO : PROGRESS: at 27.02% examples, 486192 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-07 20:04:29,091 : INFO : PROGRESS: at 27.90% examples, 478414 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:30,092 : INFO : PROGRESS: at 29.14% examples, 476638 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:31,099 : INFO : PROGRESS: at 30.27% examples, 473660 words/s, in_qsize 5, out_qsize 0\n",
      "2017-12-07 20:04:32,101 : INFO : PROGRESS: at 31.71% examples, 475536 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:33,121 : INFO : PROGRESS: at 32.99% examples, 474962 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-07 20:04:34,157 : INFO : PROGRESS: at 34.34% examples, 474689 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:35,161 : INFO : PROGRESS: at 35.82% examples, 477070 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:36,174 : INFO : PROGRESS: at 36.95% examples, 474781 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:37,176 : INFO : PROGRESS: at 38.23% examples, 474263 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:38,180 : INFO : PROGRESS: at 39.78% examples, 477093 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:39,191 : INFO : PROGRESS: at 41.26% examples, 478945 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:40,193 : INFO : PROGRESS: at 42.78% examples, 481305 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:41,198 : INFO : PROGRESS: at 44.37% examples, 484319 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:42,206 : INFO : PROGRESS: at 45.92% examples, 486446 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:43,210 : INFO : PROGRESS: at 47.45% examples, 488406 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:44,214 : INFO : PROGRESS: at 48.94% examples, 489604 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:45,220 : INFO : PROGRESS: at 50.40% examples, 490508 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:46,230 : INFO : PROGRESS: at 51.55% examples, 488465 words/s, in_qsize 6, out_qsize 0\n",
      "2017-12-07 20:04:47,242 : INFO : PROGRESS: at 52.82% examples, 487794 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:48,255 : INFO : PROGRESS: at 54.07% examples, 486674 words/s, in_qsize 5, out_qsize 2\n",
      "2017-12-07 20:04:49,253 : INFO : PROGRESS: at 55.26% examples, 485388 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:50,256 : INFO : PROGRESS: at 56.32% examples, 483041 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:51,266 : INFO : PROGRESS: at 57.68% examples, 483130 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:52,276 : INFO : PROGRESS: at 59.33% examples, 485751 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:53,292 : INFO : PROGRESS: at 61.02% examples, 488532 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-07 20:04:54,293 : INFO : PROGRESS: at 62.67% examples, 490827 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:55,302 : INFO : PROGRESS: at 64.30% examples, 493052 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:56,305 : INFO : PROGRESS: at 65.92% examples, 494961 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:57,325 : INFO : PROGRESS: at 67.55% examples, 496808 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:04:58,315 : INFO : PROGRESS: at 69.20% examples, 498678 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:04:59,328 : INFO : PROGRESS: at 70.78% examples, 500061 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:05:00,329 : INFO : PROGRESS: at 72.33% examples, 501112 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:05:01,336 : INFO : PROGRESS: at 74.01% examples, 503195 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:05:02,332 : INFO : PROGRESS: at 75.63% examples, 504801 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:05:03,339 : INFO : PROGRESS: at 77.28% examples, 506525 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:05:04,358 : INFO : PROGRESS: at 78.64% examples, 506113 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:05:05,367 : INFO : PROGRESS: at 80.26% examples, 507518 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:05:06,381 : INFO : PROGRESS: at 81.89% examples, 508796 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:05:07,394 : INFO : PROGRESS: at 83.41% examples, 509568 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:05:08,407 : INFO : PROGRESS: at 84.61% examples, 508275 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:05:09,413 : INFO : PROGRESS: at 86.08% examples, 508655 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:05:10,411 : INFO : PROGRESS: at 87.57% examples, 509166 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-07 20:05:11,421 : INFO : PROGRESS: at 89.08% examples, 509590 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:05:12,437 : INFO : PROGRESS: at 90.65% examples, 510389 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:05:13,438 : INFO : PROGRESS: at 92.19% examples, 511079 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:05:14,443 : INFO : PROGRESS: at 93.69% examples, 511646 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:05:15,450 : INFO : PROGRESS: at 95.58% examples, 514122 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-07 20:05:16,457 : INFO : PROGRESS: at 97.68% examples, 517716 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:05:17,467 : INFO : PROGRESS: at 99.76% examples, 521163 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-07 20:05:17,548 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-07 20:05:17,549 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-07 20:05:17,569 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-07 20:05:17,578 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-07 20:05:17,579 : INFO : training on 51167845 raw words (36356651 effective words) took 69.7s, 521485 effective words/s\n",
      "2017-12-07 20:05:17,588 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for training set\n",
      "Review 0 of 90000\n",
      "Review 10000 of 90000\n",
      "Review 20000 of 90000\n",
      "Review 30000 of 90000\n",
      "Review 40000 of 90000\n",
      "Review 50000 of 90000\n",
      "Review 60000 of 90000\n",
      "Review 70000 of 90000\n",
      "Review 80000 of 90000\n"
     ]
    }
   ],
   "source": [
    "#print \"len of sentences:\",len(sentences)\n",
    "\n",
    "#print sentences[0]\n",
    "\n",
    "\n",
    "##### Training the word2vec model\n",
    "\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "print( \"Training model...\")\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "#model_name = \"300features_40minwords_10context\"\n",
    "#model.save(model_name)\n",
    "\n",
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "print (\"Creating average feature vecs for training set\")\n",
    "#clean_train_reviews = review_to_sentences( tr_data, tokenizer, \\\n",
    "#        remove_stopwords=True )\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( sentences, model, num_features )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test set\n",
      "Done with 10000 Reviews\n",
      "Review 0 of 10000\n"
     ]
    }
   ],
   "source": [
    "#fte_d = codecs.open('testdatawv','r',encoding='utf-8')\n",
    "#te_data   = fte_d.read()\n",
    "test_data=X_te.values\n",
    "print (\"Creating average feature vecs for test set\")\n",
    "clean_test_reviews = review_to_sentences( X_te.values, tokenizer, \\\n",
    "        remove_stopwords=True )\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00025731, -0.01680092,  0.00895932, ..., -0.0142184 ,\n",
       "        -0.01067297, -0.00277135],\n",
       "       [ 0.00044126, -0.00732648,  0.00196551, ..., -0.00181366,\n",
       "        -0.01030169,  0.02095335],\n",
       "       [ 0.01188463, -0.0180675 , -0.00353899, ...,  0.0101138 ,\n",
       "         0.01402176, -0.01768884],\n",
       "       ..., \n",
       "       [ 0.00611622, -0.00618228, -0.008958  , ..., -0.01163218,\n",
       "        -0.01446202,  0.01348171],\n",
       "       [ 0.01544579, -0.00058106, -0.00848628, ...,  0.00437743,\n",
       "        -0.00975345, -0.00526944],\n",
       "       [-0.00898447, -0.0242632 ,  0.00608002, ...,  0.00534351,\n",
       "        -0.00244264,  0.00706471]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n",
      "90000\n",
      "10000\n",
      "train done\n",
      "test done\n",
      "10000\n",
      "Test set accuracy= 0.63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "listlabels = Y_tr.to_string().strip().split(\"\\n\")\n",
    "print len(listlabels)\n",
    "print len(testDataVecs)\n",
    "forest = forest.fit( trainDataVecs, Y_tr )\n",
    "print \"train done\"\n",
    "# Test & extract results \n",
    "result = forest.predict( testDataVecs[:1000] )\n",
    "print \"test done\"\n",
    "#Make a list of the ids\n",
    "#fTest_ids = open('testlabel','r')#,encoding='utf=8')\n",
    "#all_ids = fTest_ids.read()\n",
    "#fTest_ids.close()\n",
    "idsList = Y_te.to_string().strip().split(\"\\n\")\n",
    "print len(idsList)\n",
    "# Write the test results \n",
    "#output = []\n",
    "#output = pd.DataFrame( data={\"id\":idsList, \"prediction:\":result, \"gold-label:\":labelslist} )\n",
    "#output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )\n",
    "acc = 0.0\n",
    "for a,b in zip(result,Y_te):\n",
    "    if a == b:\n",
    "        acc += 1.0\n",
    "\n",
    "print \"Test set accuracy=\", acc / len(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listlabels = Y_tr.to_string().strip().split(\"\\n\")\n",
    "idsList = Y_te.to_string().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = forest.fit( trainDataVecs,Y_tr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre=forest.predict( testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 1 ..., 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "print pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.70      0.69      0.69      1503\n",
      "          2       0.53      0.07      0.13       992\n",
      "          4       0.50      0.29      0.36      2686\n",
      "          5       0.63      0.89      0.74      4819\n",
      "\n",
      "avg / total       0.60      0.62      0.57     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(Y_te, pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
